{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Linear Algebra\n",
    "\n",
    "## Solving Matrix Equations\n",
    "\n",
    "Consider the system of linear equations\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "a x + b y &= p, \\\\\n",
    "c x + d y &= q.\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "We can write this as the matrix equation\n",
    "\\begin{equation}\n",
    "\\mathbf{M}\\, \\vec{x} = \\vec{r},\n",
    "\\end{equation}\n",
    "or more explicitly,\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "a & b \\\\ c & d \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} =\n",
    "\\begin{bmatrix} p \\\\ q \\end{bmatrix}.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elementary Row Operations\n",
    "\n",
    "To solve matrix equations of the form shown above, we need to make use of the three elementary row operations:\n",
    "<ol>\n",
    "<li> interchange of two rows $R_i, R_j \\to R_j, R_i$;\n",
    "<li> scaling of a row $R_i \\to \\alpha R_i$;\n",
    "<li> addition of two rows $R_i \\to R_i + R_j$;\n",
    "</ol>\n",
    "\n",
    "These operations do not change the solution of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Elimination\n",
    "\n",
    "Gaussian elimination make use of successive elementary row operations to solve a given matrix equation. The goal is to convert the matrix $\\mathbf{M}$ into an identity matrix, or to a <em>reduced row echelon form</em>, or even just to <em>row echelon form</em>.\n",
    "\n",
    "For example, the matrix\n",
    "\\begin{equation}\n",
    "\\mathbf{P} = \\begin{bmatrix}\n",
    "1 & p_{12} & p_{13} & \\cdots & p_{1N} \\\\\n",
    "0 & 1 & p_{23} & \\cdots & p_{2N} \\\\\n",
    "0 & 0 & 1 & \\cdots & p_{3N} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & \\cdots & 1 \\end{bmatrix}\n",
    "\\end{equation}\n",
    "is of reduced row echelon form, because the diagonals are all 1, the lower triangle of the matrix are all zeros, the non-zero matrix elements are in the upper triangle. A matrix that has only zero matrix elements in the lower triangle, but whose diagonals are not ones, is called to have a <em>row echelon form</em>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To see how this works, consider the matrix equation $\\mathbf{A}\\vec{x} = \\vec{b}$, which we write more explicitly as\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "2 & -3 & 1 & 3 \\\\\n",
    "1 & 4 & -3 & -3 \\\\\n",
    "5 & 3 & -1 & -1 \\\\\n",
    "3 & -6 & -3 & 1 \\end{bmatrix}\n",
    "\\begin{bmatrix} w \\\\ x \\\\ y \\\\ z \\end{bmatrix} =\n",
    "\\begin{bmatrix} -4 \\\\ 1 \\\\ 8 \\\\ -5 \\end{bmatrix}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 2., -3.,  1.,  3.],\n",
       "        [ 1.,  4., -3., -3.],\n",
       "        [ 5.,  3., -1., -1.],\n",
       "        [ 3., -6., -3.,  1.]]), array([[-4.],\n",
       "        [ 1.],\n",
       "        [ 8.],\n",
       "        [-5.]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([[2.0,-3.0,1.0,3.0],[1.0,4.0,-3.0,-3.0],[5.0,3.0,-1.0,-1.0],[3.0,-6.0,-3.0,1.0]])\n",
    "b = np.array([[-4.0],[1.0],[8.0],[-5.0]])\n",
    "A, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First construct the <em>augmented matrix</em>,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2., -3.,  1.,  3., -4.],\n",
       "       [ 1.,  4., -3., -3.,  1.],\n",
       "       [ 5.,  3., -1., -1.,  8.],\n",
       "       [ 3., -6., -3.,  1., -5.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ab = np.concatenate((A, b), axis=1)2\n",
    "Ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us do Gaussian elimination without pivoting to the identity matrix. In fact, we can do this without any interchange of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us rescale the first row, to make its leading element 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. , -1.5,  0.5,  1.5, -2. ],\n",
       "       [ 1. ,  4. , -3. , -3. ,  1. ],\n",
       "       [ 5. ,  3. , -1. , -1. ,  8. ],\n",
       "       [ 3. , -6. , -3. ,  1. , -5. ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ab[0,:] = Ab[0,:]/2.0\n",
    "Ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we subtract an appropriate multiple of the first row from the subsequent rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1. ,  -1.5,   0.5,   1.5,  -2. ],\n",
       "       [  0. ,   5.5,  -3.5,  -4.5,   3. ],\n",
       "       [  0. ,  10.5,  -3.5,  -8.5,  18. ],\n",
       "       [  0. ,  -1.5,  -4.5,  -3.5,   1. ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ab[1,:] = Ab[1,:] - Ab[0,:]\n",
    "Ab[2,:] = Ab[2,:] - 5.0*Ab[0,:]\n",
    "Ab[3,:] = Ab[3,:] - 3.0*Ab[0,:]\n",
    "Ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this, we rescale the second row, so that its leading element is 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.        ,  -1.5       ,   0.5       ,   1.5       ,  -2.        ],\n",
       "       [  0.        ,   1.        ,  -0.63636364,  -0.81818182,\n",
       "          0.54545455],\n",
       "       [  0.        ,  10.5       ,  -3.5       ,  -8.5       ,  18.        ],\n",
       "       [  0.        ,  -1.5       ,  -4.5       ,  -3.5       ,   1.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ab[1,:] = Ab[1,:]/5.5\n",
    "Ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then subtract appropriate multiples of row 2 from row 3 and row 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.        ,  -1.5       ,   0.5       ,   1.5       ,  -2.        ],\n",
       "       [  0.        ,   1.        ,  -0.63636364,  -0.81818182,\n",
       "          0.54545455],\n",
       "       [  0.        ,   0.        ,   3.18181818,   0.09090909,\n",
       "         12.27272727],\n",
       "       [  0.        ,   0.        ,  -5.45454545,  -4.72727273,\n",
       "          1.81818182]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ab[2,:] = Ab[2,:] - 10.5*Ab[1,:]\n",
    "Ab[3,:] = Ab[3,:] + 1.5*Ab[1,:]\n",
    "Ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing, we rescale row 3 so that its leading element is 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -1.5       ,  0.5       ,  1.5       , -2.        ],\n",
       "       [ 0.        ,  1.        , -0.63636364, -0.81818182,  0.54545455],\n",
       "       [ 0.        ,  0.        ,  1.        ,  0.02857143,  3.85714286],\n",
       "       [ 0.        ,  0.        , -5.45454545, -4.72727273,  1.81818182]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ab[2,:] = Ab[2,:]/Ab[2,2]\n",
    "Ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And subtract an appropriate multiple of row 3 from row 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.        ,  -1.5       ,   0.5       ,   1.5       ,  -2.        ],\n",
       "       [  0.        ,   1.        ,  -0.63636364,  -0.81818182,\n",
       "          0.54545455],\n",
       "       [  0.        ,   0.        ,   1.        ,   0.02857143,\n",
       "          3.85714286],\n",
       "       [  0.        ,   0.        ,   0.        ,  -4.57142857,\n",
       "         22.85714286]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ab[3,:] = Ab[3,:] - Ab[3,2]*Ab[2,:]\n",
    "Ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then rescale row 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -1.5       ,  0.5       ,  1.5       , -2.        ],\n",
       "       [ 0.        ,  1.        , -0.63636364, -0.81818182,  0.54545455],\n",
       "       [ 0.        ,  0.        ,  1.        ,  0.02857143,  3.85714286],\n",
       "       [-0.        , -0.        , -0.        ,  1.        , -5.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ab[3,:] = Ab[3,:]/Ab[3,3]\n",
    "Ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The augmented matrix is now in reduced row echelon form. If we wish, we can already solve for $[w, x, y, z]$ through backward substitution. But let us proceed to make the first part of the augmented matrix an identity matrix. This is done by subtracting an appropriate multiple of row 4 from row 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -1.5       ,  0.5       ,  1.5       , -2.        ],\n",
       "       [ 0.        ,  1.        , -0.63636364, -0.81818182,  0.54545455],\n",
       "       [ 0.        ,  0.        ,  1.        ,  0.        ,  4.        ],\n",
       "       [-0.        , -0.        , -0.        ,  1.        , -5.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ab[2,:] = Ab[2,:] - Ab[2,3]*Ab[3,:]\n",
    "Ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "followed by subtracting an appropriate multiple of row 4 from row 2, and an appropriate multiple of row 3 from row 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. , -1.5,  0.5,  1.5, -2. ],\n",
       "       [ 0. ,  1. ,  0. ,  0. , -1. ],\n",
       "       [ 0. ,  0. ,  1. ,  0. ,  4. ],\n",
       "       [-0. , -0. , -0. ,  1. , -5. ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ab[1,:] = Ab[1,:] - Ab[1,3]*Ab[3,:]\n",
    "Ab[1,:] = Ab[1,:] - Ab[1,2]*Ab[2,:]\n",
    "Ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we subtract appropriate multiples of row 4, row 3, and row 2 from row 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  2.],\n",
       "       [ 0.,  1.,  0.,  0., -1.],\n",
       "       [ 0.,  0.,  1.,  0.,  4.],\n",
       "       [-0., -0., -0.,  1., -5.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ab[0,:] = Ab[0,:] - Ab[0,3]*Ab[3,:]\n",
    "Ab[0,:] = Ab[0,:] - Ab[0,2]*Ab[2,:]\n",
    "Ab[0,:] = Ab[0,:] - Ab[0,1]*Ab[1,:]\n",
    "Ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ab[3,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final form of the augmented matrix, we then read off the solution as $w = 2$, $x = -1$, $y = 4$, and $z = -5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Elimination with Pivoting\n",
    "\n",
    "In the above example, we went through the Gaussian elimination process without any row interchanges. When the system of linear equations is large, involving $N \\gg 1$ variables, or when the system of linear equations is <em>stiff</em>, it is generally advisable to perform pivoting during the Gaussian elimination.\n",
    "\n",
    "Let us try the same example again, with pivoting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2., -3.,  1.,  3., -4.],\n",
       "       [ 1.,  4., -3., -3.,  1.],\n",
       "       [ 5.,  3., -1., -1.,  8.],\n",
       "       [ 3., -6., -3.,  1., -5.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ab = np.concatenate((A, b), axis=1)\n",
    "Ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To performing pivoting, we must ensure that when we are rescaling the leading matrix element, we rescale the row with the largest absolute matrix element. In this case, we should make $[5, 3, -1, -1, 8]$ the first row, and rescale it by 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  3., -1., -1.,  8.],\n",
       "       [ 1.,  4., -3., -3.,  1.],\n",
       "       [ 2., -3.,  1.,  3., -4.],\n",
       "       [ 3., -6., -3.,  1., -5.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ab[[0,2]] = Ab[[2,0]]\n",
    "Ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. ,  0.6, -0.2, -0.2,  1.6],\n",
       "       [ 1. ,  4. , -3. , -3. ,  1. ],\n",
       "       [ 2. , -3. ,  1. ,  3. , -4. ],\n",
       "       [ 3. , -6. , -3. ,  1. , -5. ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ab[0,:] = Ab[0,:]/Ab[0,0]\n",
    "Ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is done, we subtract appropriate multiples of row 1 from rows 2, 3, 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. ,  0.6, -0.2, -0.2,  1.6],\n",
       "       [ 0. ,  3.4, -2.8, -2.8, -0.6],\n",
       "       [ 0. , -4.2,  1.4,  3.4, -7.2],\n",
       "       [ 0. , -7.8, -2.4,  1.6, -9.8]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ab[1,:] = Ab[1,:] - Ab[1,0]*Ab[0,:]\n",
    "Ab[2,:] = Ab[2,:] - Ab[2,0]*Ab[0,:]\n",
    "Ab[3,:] = Ab[3,:] - Ab[3,0]*Ab[0,:]\n",
    "Ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we rescale the leading element of the second row. To pivot, we must first interchange row 2 and row 4, since $|-7.8|$ is the largest in magnitude:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. ,  0.6, -0.2, -0.2,  1.6],\n",
       "       [ 0. , -7.8, -2.4,  1.6, -9.8],\n",
       "       [ 0. , -4.2,  1.4,  3.4, -7.2],\n",
       "       [ 0. ,  3.4, -2.8, -2.8, -0.6]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ab[[1,3]] = Ab[[3,1]]\n",
    "Ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.6       , -0.2       , -0.2       ,  1.6       ],\n",
       "       [-0.        ,  1.        ,  0.30769231, -0.20512821,  1.25641026],\n",
       "       [ 0.        , -4.2       ,  1.4       ,  3.4       , -7.2       ],\n",
       "       [ 0.        ,  3.4       , -2.8       , -2.8       , -0.6       ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ab[1,:] = Ab[1,:]/Ab[1,1]\n",
    "Ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To proceed, we subtract appropriate multiples of row 2 from rows 3 and 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.6       , -0.2       , -0.2       ,  1.6       ],\n",
       "       [-0.        ,  1.        ,  0.30769231, -0.20512821,  1.25641026],\n",
       "       [ 0.        ,  0.        ,  2.69230769,  2.53846154, -1.92307692],\n",
       "       [ 0.        ,  0.        , -3.84615385, -2.1025641 , -4.87179487]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ab[2,:] = Ab[2,:] - Ab[2,1]*Ab[1,:]\n",
    "Ab[3,:] = Ab[3,:] - Ab[3,1]*Ab[1,:]\n",
    "Ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next rescaling, we interchange row 3 and row 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.6       , -0.2       , -0.2       ,  1.6       ],\n",
       "       [-0.        ,  1.        ,  0.30769231, -0.20512821,  1.25641026],\n",
       "       [ 0.        ,  0.        , -3.84615385, -2.1025641 , -4.87179487],\n",
       "       [ 0.        ,  0.        ,  2.69230769,  2.53846154, -1.92307692]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ab[[2,3]] = Ab[[3,2]]\n",
    "Ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.6       , -0.2       , -0.2       ,  1.6       ],\n",
       "       [-0.        ,  1.        ,  0.30769231, -0.20512821,  1.25641026],\n",
       "       [-0.        , -0.        ,  1.        ,  0.54666667,  1.26666667],\n",
       "       [ 0.        ,  0.        ,  2.69230769,  2.53846154, -1.92307692]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ab[2,:] = Ab[2,:]/Ab[2,2]\n",
    "Ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost there! Subtracting an appropriate multiple of row 3 from row 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.6       , -0.2       , -0.2       ,  1.6       ],\n",
       "       [-0.        ,  1.        ,  0.30769231, -0.20512821,  1.25641026],\n",
       "       [-0.        , -0.        ,  1.        ,  0.54666667,  1.26666667],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.06666667, -5.33333333]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ab[3,:] = Ab[3,:] - Ab[3,2]*Ab[2,:]\n",
    "Ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we rescale row 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.6       , -0.2       , -0.2       ,  1.6       ],\n",
       "       [-0.        ,  1.        ,  0.30769231, -0.20512821,  1.25641026],\n",
       "       [-0.        , -0.        ,  1.        ,  0.54666667,  1.26666667],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.        , -5.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ab[3,:] = Ab[3,:]/Ab[3,3]\n",
    "Ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we end up with a (different) reduced row echelon form.\n",
    "\n",
    "From this augmented matrix, we read off the solution for $z$ from the last row as $z = -5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third row of this augmented matrix tells us that $y + 0.546666667 z = 1.26666667$. Therefore, to solve for $y$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0000000000000009"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ab[2,4] - Ab[2,3]*Ab[3,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is indeed what we get from the previous Gaussian elimination.\n",
    "\n",
    "Moving on, the second row of the augmented matrix tells us that\n",
    "\\begin{equation}\n",
    "x + 0.30769231 y - 0.20512821 z = 1.25641026,\n",
    "\\end{equation}\n",
    "and therefore $x$ is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ab[1,4] - Ab[1,2]*4 - Ab[1,3]*(-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without writing out the equation for $w$, we can see that its solution must be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ab[0,4] - Ab[0,1]*(-1.0) - Ab[0,2]*(4.0) - Ab[0,3]*(-5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving a Matrix Equation Using the Numpy Solver\n",
    "\n",
    "To ensure that you do not become button-pushing technicians, I have shown you the basic principles behind Gaussian elimination in solving matrix equations.\n",
    "\n",
    "Naturally, for implementing implicit schemes to solve PDEs, you should not need to do the Gaussian elimination manually as shown above, or even write your own Gaussian elimination solver.\n",
    "\n",
    "Instead, you should use the built-in solver in numpy, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.],\n",
       "       [-1.],\n",
       "       [ 4.],\n",
       "       [-5.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.solve(A,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Methods for Solving Matrix Equations\n",
    "\n",
    "### Cramers Method\n",
    "\n",
    "Most of you would have learnt Cramers method for solving matrix equations. In the Cramers method, you compute <em>determinants</em> instead of performing Gaussian elimination.\n",
    "\n",
    "For a $N \\times N$ matrix\n",
    "\\begin{equation}\n",
    "\\mathbf{A} = \\begin{bmatrix}\n",
    "A_{11} & A_{12} & A_{13} & \\cdots & A_{1N} \\\\\n",
    "A_{21} & A_{22} & A_{23} & \\cdots & A_{2N} \\\\\n",
    "A_{31} & A_{32} & A_{33} & \\cdots & A_{3N} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "A_{N1} & A_{N2} & A_{N3} & \\cdots & A_{NN}\n",
    "\\end{bmatrix},\n",
    "\\end{equation}\n",
    "its determinant is defined as\n",
    "\\begin{equation}\n",
    "\\det\\mathbf{A} = |\\mathbf{A}| = \\sum_{\\pi} (-1)^{\\pi} \\prod_{i=1}^N A_{i,\\pi(i)},\n",
    "\\end{equation}\n",
    "where the sum is over all permutations $\\pi$ of $(1, 2, \\dots, N)$, and\n",
    "\\begin{equation}\n",
    "(-1)^{\\pi} = \\begin{cases}\n",
    "1, & \\text{$\\pi$ is an even permutation}; \\\\\n",
    "-1, & \\text{$\\pi$ is an odd permutation}. \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "For example, $(2, 3, 4, 1)$ is an odd permutation of $(1, 2, 3, 4)$, because\n",
    "\\begin{equation}\n",
    "(2, 3, 4, 1) \\to (2, 3, 1, 4) \\to (2, 1, 3, 4) \\to (1, 2, 3, 4).\n",
    "\\end{equation}\n",
    "\n",
    "On the other hand, $(3, 1, 2, 4)$ is an even permutation of $(1, 2, 3, 4)$ because\n",
    "\\begin{equation}\n",
    "(3, 1, 2, 4) \\to (1, 3, 2, 4) \\to (1, 2, 3, 4).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, in a Computational Physics class, I do not expect you to evaluate the determinant of a matrix starting from the definition, but to use the built-in numpy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-159.99999999999994"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the Cramers method to the matrix equation $\\mathbf{A}\\vec{x} = \\vec{b}$, we need to first evaluate five determinants.\n",
    "\n",
    "The first is the determinant of $\\mathbf{A}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-159.99999999999994"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dA = np.linalg.det(A)\n",
    "dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next determinant we need to evaluate is when we replace the first column of $\\mathbf{A}$ by $\\vec{b}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4., -3.,  1.,  3.],\n",
       "       [ 1.,  4., -3., -3.],\n",
       "       [ 8.,  3., -1., -1.],\n",
       "       [-5., -6., -3.,  1.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Aw = np.concatenate((b,A[:,1:4]),axis=1)\n",
    "Aw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-319.99999999999994"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dAw = np.linalg.det(Aw)\n",
    "dAw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third determinant we need to evaluate is when we replace the second column of $\\mathbf{A}$ by $\\vec{b}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2., -3.,  1.,  3.],\n",
       "       [ 1.,  4., -3., -3.],\n",
       "       [ 5.,  3., -1., -1.],\n",
       "       [ 3., -6., -3.,  1.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ax = np.array(A)\n",
    "Ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2., -4.,  1.,  3.],\n",
       "       [ 1.,  1., -3., -3.],\n",
       "       [ 5.,  8., -1., -1.],\n",
       "       [ 3., -5., -3.,  1.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ax[:,1:2] = b\n",
    "Ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159.99999999999994"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dAx = np.linalg.det(Ax)\n",
    "dAx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2., -3., -4.,  3.],\n",
       "       [ 1.,  4.,  1., -3.],\n",
       "       [ 5.,  3.,  8., -1.],\n",
       "       [ 3., -6., -5.,  1.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ay = np.array(A)\n",
    "Ay[:,2:3] = b\n",
    "Ay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-639.99999999999989"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dAy = np.linalg.det(Ay)\n",
    "dAy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2., -3.,  1., -4.],\n",
       "       [ 1.,  4., -3.,  1.],\n",
       "       [ 5.,  3., -1.,  8.],\n",
       "       [ 3., -6., -3., -5.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Az = np.array(A)\n",
    "Az[:,3:4] = b\n",
    "Az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799.99999999999989"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dAz = np.linalg.det(Az)\n",
    "dAz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, Cramers method tells us that the solution is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0000000000000004"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = dAw/dA\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = dAx/dA\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0000000000000009"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = dAy/dA\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.0000000000000009"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = dAz/dA\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Remark:</b> The Cramers method is not suitable for solving large matrix equations, because of the large number of determinants that must first be evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LU Decomposition\n",
    "\n",
    "If we have to solve the matrix equation $\\mathbf{A}\\vec{x} = \\vec{b}$ once, then Gaussian elimination would be fine.\n",
    "\n",
    "However, if we have to keep solving the matrix equation $\\mathbf{A}\\vec{x} = \\vec{b}_i$, for $i = 1, 2, \\dots, M$, then it would be better to first perform an <em>LU Decomposition</em>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a LU Decomposition, we write the $N \\times N$ matrix\n",
    "\\begin{equation}\n",
    "\\mathbf{A} = \\mathbf{L}\\mathbf{U}\n",
    "\\end{equation}\n",
    "as the product of two $N \\times N$ matrices $\\mathbf{L}$ and $\\mathbf{U}$.\n",
    "\n",
    "The matrix\n",
    "\\begin{equation}\n",
    "\\mathbf{L} = \\begin{bmatrix}\n",
    "L_{11} & 0 & 0 & \\cdots & 0 \\\\\n",
    "L_{21} & L_{22} & 0 & \\cdots & 0 \\\\\n",
    "L_{31} & L_{32} & L_{33} & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "L_{N1} & L_{N2} & L_{N3} & \\cdots & L_{NN} \\end{bmatrix}\n",
    "\\end{equation}\n",
    "is a <em>lower triangular</em> matrix, whereas the matrix\n",
    "\\begin{equation}\n",
    "\\mathbf{U} = \\begin{bmatrix}\n",
    "U_{11} & U_{12} & U_{13} & \\cdots & U_{1N} \\\\\n",
    "0 & U_{22} & U_{23} & \\cdots & U_{2N} \\\\\n",
    "0 & 0 & U_{33} & \\cdots & U_{3N} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & \\cdots & U_{NN} \\end{bmatrix}\n",
    "\\end{equation}\n",
    "is an <em>upper triangular</em> matrix.\n",
    "\n",
    "In terms of $\\mathbf{L}$ and $\\mathbf{U}$, the original matrix equation can be written as\n",
    "\\begin{equation}\n",
    "\\mathbf{A}\\vec{x} = \\mathbf{L}\\mathbf{U}\\vec{x} = \\mathbf{L}\\vec{y} = \\vec{b}.\n",
    "\\end{equation}\n",
    "This tells us that we can solve for $\\vec{y} = (y_1, y_2, \\dots, y_N)$ from $\\mathbf{L}\\vec{y} = \\vec{b}$ using forward substutition.\n",
    "\n",
    "Once this is done, we can solve for $\\vec{x} = (x_1, x_2, \\dots, x_N)$ from $\\mathbf{U}\\vec{x} = \\vec{y}$ using backward substitution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Cholesky Decomposition\n",
    "\n",
    "If $\\mathbf{A}$ is symmetric, i.e. $\\mathbf{A}^T = \\mathbf{A}$, then \n",
    "\\begin{equation}\n",
    "\\mathbf{A} = \\mathbf{L}\\mathbf{L}^T,\n",
    "\\end{equation}\n",
    "making the decomposition of $\\mathbf{A}$ into a lower triangular matrix $\\mathbf{L}$ and an upper triangular matrix $\\mathbf{L}^T$ easier.\n",
    "\n",
    "This decomposition is then known as <em>Cholesky decomposition</em>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvalue Problem\n",
    "\n",
    "The other matrix problem we commonly encounter in computational physics is the <em>eigenvalue problem</em>,\n",
    "\\begin{equation}\n",
    "\\mathbf{A}\\vec{x} = \\lambda\\vec{x},\n",
    "\\end{equation}\n",
    "where $\\mathbf{A}$ is a given matrix, and we are to solve for possible pairs of $\\vec{x}$ (the <em>eigenvector</em>) and $\\lambda$ (the <em>eigenvalue</em>).\n",
    "\n",
    "In general, if $\\mathbf{A}$ is $N \\times N$, we can find $N$ eigenvectors $\\vec{x}_i$ and their $N$ eigenvalues $\\lambda_i$. Sometimes, it is also possible to find fewer than $N$ eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use\n",
    "\\begin{equation}\n",
    "\\mathbf{A} = \\begin{bmatrix}\n",
    "2 & -3 & 1 & 3 \\\\\n",
    "1 & 4 & -3 & -3 \\\\\n",
    "5 & 3 & -1 & -1 \\\\\n",
    "3 & -6 & -3 & 1 \\end{bmatrix}\n",
    "\\end{equation}\n",
    "as an example, to learn how to find its eigenvectors and eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the $N$ eigenvectors $\\vec{u}_i$ of $\\mathbf{A}$ are linearly independent, so we can write an arbitrary vector $\\vec{x}$ in terms of them as\n",
    "\\begin{equation}\n",
    "\\vec{x} = \\sum_{i=1}^N \\alpha_i \\vec{u}_i.\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, if we let $\\mathbf{A}$ act on $\\vec{x}$,\n",
    "\\begin{equation}\n",
    "\\mathbf{A}\\vec{x} = \\sum_{i=1}^N \\alpha_i \\mathbf{A}\\vec{u}_i = \\sum_{i=1}^N \\alpha_i \\lambda_i \\vec{u}_i.\n",
    "\\end{equation}\n",
    "If we multiply $\\mathbf{A}\\vec{x}$ by $\\mathbf{A}$ again, we will have\n",
    "\\begin{equation}\n",
    "\\mathbf{A}(\\mathbf{A}\\vec{x}) = \\mathbf{A}^2\\vec{x} = \\sum_{i=1}^N \\alpha_i \\lambda_i^2 \\vec{u}_i.\n",
    "\\end{equation}\n",
    "We see that, if we multiply $\\vec{x}$ by $\\mathbf{A}$ $M$ times, we will end up with\n",
    "\\begin{equation}\n",
    "\\mathbf{A}^M\\vec{x} = \\sum_{i=1}^N \\alpha_i \\lambda_i^M \\vec{u}_i.\n",
    "\\end{equation}\n",
    "\n",
    "## The Power Method\n",
    "\n",
    "In general, $\\mathbf{A}$ will have an eigenvalue $\\lambda_1$ with the largest absolute value $|\\lambda_1| > |\\lambda_i|$, $i = 2, \\dots, N$. If we pull this term out of the sum, we will have\n",
    "\\begin{equation}\n",
    "\\mathbf{A}^M\\vec{x} = \\alpha_1 \\lambda_1^M \\vec{u}_1 + \\sum_{i=2}^N \\alpha_i \\lambda_i^M \\vec{u}_i,\n",
    "\\end{equation}\n",
    "which we can rewrite as\n",
    "\\begin{equation}\n",
    "\\mathbf{A}^M\\vec{x} = \\lambda_1^M \\left[\\alpha_1 \\vec{u}_1 + \\sum_{i=2}^N \\alpha_i \\left(\\dfrac{\\lambda_i^M}{\\lambda_1^M}\\right) \\vec{u}_i\\right].\n",
    "\\end{equation}\n",
    "\n",
    "Now, $|\\lambda_i/\\lambda_1| < 1$ for all $i = 2, \\dots, N$, so if $M$ is large enough, we will have\n",
    "\\begin{equation}\n",
    "\\lim_{M \\to \\infty} \\mathbf{A}^M\\vec{x} = \\lim_{M \\to \\infty} \\lambda_1^M \\alpha_1 \\vec{u}_1.\n",
    "\\end{equation}\n",
    "In other words, whatever $\\vec{x}$ we start with, after multiplying by $\\mathbf{A}$ a large number of times, we would end up with a vector proportional to $\\vec{u}_1$, the eigenvector of $\\mathbf{A}$ associated with the eigenvalue $\\lambda_1$ with the largest absolute value.\n",
    "\n",
    "This suggests that to find $\\vec{u}_1$ and $\\lambda_1$, we start with a random $\\vec{x}_0$, and calculate iteratively\n",
    "\\begin{equation}\n",
    "\\vec{x}_{i+1} = \\dfrac{\\mathbf{A}\\vec{x}_i}{|\\mathbf{A}\\vec{x}_i|}.\n",
    "\\end{equation}\n",
    "Then $\\vec{u}_1 = \\lim_{i \\to \\infty} \\vec{x}_i$, and since $\\vec{u}_1$ will be properly normalized, we find $\\lambda_1$ by\n",
    "\\begin{equation}\n",
    "\\lambda_1 = \\vec{u}_1^T \\mathbf{A} \\vec{u}_1.\n",
    "\\end{equation}\n",
    "\n",
    "This method, which was invented during the Second World War, of finding the the eigenvector $\\vec{u}_1$ associated with the eigenvalue $\\lambda_1$ with the largest absolute value is called the <em>power method</em>.\n",
    "\n",
    "In the following, let us test the power method on the matrix $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2., -3.,  1.,  3.],\n",
       "       [ 1.,  4., -3., -3.],\n",
       "       [ 5.,  3., -1., -1.],\n",
       "       [ 3., -6., -3.,  1.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x0 = np.random.random((4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.38182897],\n",
       "       [ 0.92796689],\n",
       "       [ 0.27018296],\n",
       "       [ 0.60771336]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3171759 ],\n",
       "       [ 0.77083918],\n",
       "       [ 0.22443431],\n",
       "       [ 0.50481248]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = x0/np.linalg.norm(x0)\n",
    "x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01184206],\n",
       "       [ 0.23658211],\n",
       "       [ 0.61821326],\n",
       "       [-0.74946717]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.matmul(A,x0)\n",
    "x1 = x1/np.linalg.norm(x1)\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.47371299],\n",
       "       [ 0.27649335],\n",
       "       [ 0.18410851],\n",
       "       [-0.8156295 ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = np.matmul(A, x1)\n",
    "x2 = x2/np.linalg.norm(x2)\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.61381811],\n",
       "       [ 0.38394319],\n",
       "       [-0.1379016 ],\n",
       "       [-0.67586841]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3 = np.matmul(A, x2)\n",
    "x3 = x3/np.linalg.norm(x3)\n",
    "x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.62660906],\n",
       "       [ 0.46368863],\n",
       "       [-0.15213677],\n",
       "       [-0.60762517]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x4 = np.matmul(A, x3)\n",
    "x4 = x4/np.linalg.norm(x4)\n",
    "x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.60774235],\n",
       "       [ 0.46145871],\n",
       "       [-0.12922643],\n",
       "       [-0.63325005]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x5 = np.matmul(A, x4)\n",
    "x5 = x5/np.linalg.norm(x5)\n",
    "x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.60751996],\n",
       "       [ 0.46271336],\n",
       "       [-0.11705363],\n",
       "       [-0.63491282]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x6 = np.matmul(A, x5)\n",
    "x6 = x6/np.linalg.norm(x6)\n",
    "x6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[-0.60582699]\n",
      " [ 0.45836594]\n",
      " [-0.11756301]\n",
      " [-0.63957272]]\n",
      "1 [[-0.60710237]\n",
      " [ 0.45949876]\n",
      " [-0.11778219]\n",
      " [-0.63750683]]\n",
      "2 [[-0.60673667]\n",
      " [ 0.45892508]\n",
      " [-0.11834526]\n",
      " [-0.63816361]]\n",
      "3 [[-0.60697573]\n",
      " [ 0.45932507]\n",
      " [-0.11821555]\n",
      " [-0.63767236]]\n",
      "4 [[-0.60684127]\n",
      " [ 0.45915001]\n",
      " [-0.11826835]\n",
      " [-0.63791656]]\n",
      "5 [[-0.60690234]\n",
      " [ 0.45923978]\n",
      " [-0.11822217]\n",
      " [-0.63780239]]\n",
      "6 [[-0.6068695 ]\n",
      " [ 0.45919063]\n",
      " [-0.11824148]\n",
      " [-0.63786545]]\n",
      "7 [[-0.60688673]\n",
      " [ 0.45921507]\n",
      " [-0.11823195]\n",
      " [-0.63783322]]\n",
      "8 [[-0.60687809]\n",
      " [ 0.4592025 ]\n",
      " [-0.11823739]\n",
      " [-0.63784949]]\n",
      "9 [[-0.60688258]\n",
      " [ 0.45920905]\n",
      " [-0.11823471]\n",
      " [-0.637841  ]]\n",
      "10 [[-0.60688026]\n",
      " [ 0.4592057 ]\n",
      " [-0.11823608]\n",
      " [-0.63784536]]\n",
      "11 [[-0.60688145]\n",
      " [ 0.45920742]\n",
      " [-0.11823536]\n",
      " [-0.63784312]]\n",
      "12 [[-0.60688083]\n",
      " [ 0.45920654]\n",
      " [-0.11823573]\n",
      " [-0.63784428]]\n",
      "13 [[-0.60688115]\n",
      " [ 0.45920699]\n",
      " [-0.11823554]\n",
      " [-0.63784368]]\n",
      "14 [[-0.60688099]\n",
      " [ 0.45920676]\n",
      " [-0.11823564]\n",
      " [-0.63784399]]\n",
      "15 [[-0.60688107]\n",
      " [ 0.45920688]\n",
      " [-0.11823559]\n",
      " [-0.63784383]]\n",
      "16 [[-0.60688103]\n",
      " [ 0.45920682]\n",
      " [-0.11823561]\n",
      " [-0.63784391]]\n",
      "17 [[-0.60688105]\n",
      " [ 0.45920685]\n",
      " [-0.1182356 ]\n",
      " [-0.63784387]]\n",
      "18 [[-0.60688104]\n",
      " [ 0.45920683]\n",
      " [-0.11823561]\n",
      " [-0.63784389]]\n",
      "19 [[-0.60688104]\n",
      " [ 0.45920684]\n",
      " [-0.1182356 ]\n",
      " [-0.63784388]]\n"
     ]
    }
   ],
   "source": [
    "for n in range(20):\n",
    "    x6 = np.matmul(A, x6)\n",
    "    x6 = x6/np.linalg.norm(x6)\n",
    "    print(n, x6)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After more than 20 multiplications of the random initial vector $\\vec{x}_0$ by $\\mathbf{A}$, we end up with $\\vec{u}_1 = [-0.606881, 0.459207, -0.118236, -0.637844]^T$, which is correct up to at least the sixth decimal place.\n",
    "\n",
    "Next, we compute the eigenvalue $\\lambda_1$ as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.61788479]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(np.transpose(x6), (np.matmul(A, x6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The Inverse Power Method\n",
    "\n",
    "As we have seen, the power method allows us to find the eigenvector $\\vec{u}_1$ of the matrix $\\mathbf{A}$ associated with the eigenvalue $\\lambda_1$ with the largest absolute value.\n",
    "\n",
    "But what if we are not interested in this eigenvector?\n",
    "\n",
    "What if we are interested in the eigenvector associated with the eigenvalue with the smallest absolute value?\n",
    "\n",
    "To modify the power method to find this, let us first observe that $\\mathbf{A}^{-1}$ shares the same eigenvectors $\\{\\vec{u}_i\\}_{i=1}^N$ as $\\mathbf{A}$. To show this, let us start from\n",
    "\\begin{equation}\n",
    "\\mathbf{A}\\vec{u}_i = \\lambda_i \\vec{u}_i,\n",
    "\\end{equation}\n",
    "where $\\vec{u}_i$ is the eigenvector of $\\mathbf{A}$ with eigenvalue $\\lambda_i$. Next, we multiply both sides of the equation by $\\mathbf{A}^{-1}$:\n",
    "\\begin{equation}\n",
    "\\mathbf{A}^{-1}\\mathbf{A}\\vec{u}_i = \\lambda_i \\mathbf{A}^{-1}\\vec{u}_i.\n",
    "\\end{equation}\n",
    "Since $\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}$, we therefore have\n",
    "\\begin{equation}\n",
    "\\vec{u}_i = \\lambda_i \\mathbf{A}^{-1}\\vec{u}_i.\n",
    "\\end{equation}\n",
    "Dividing throughout by $\\lambda_i$, and making $\\mathbf{A}^{-1}\\vec{u}_i$ the subject, we finally have\n",
    "\\begin{equation}\n",
    "\\mathbf{A}^{-1}\\vec{u}_i = \\dfrac{1}{\\lambda_i}\\vec{u}_i,\n",
    "\\end{equation}\n",
    "which tells us that $\\vec{u}_i$ is an eigenvector of $\\mathbf{A}^{-1}$ with eigenvalue $\\lambda_i^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us pretend that we have worked out $\\mathbf{A}^{-1}$, and write the $(i+1)$th iterate $\\vec{x}_{i+1}$ in terms of the $i$th iterate $\\vec{x}_i$ as\n",
    "\\begin{equation}\n",
    "\\vec{x}_{i+1} = \\mathbf{A}^{-1}\\vec{x}_i,\n",
    "\\end{equation}\n",
    "then we know that after multiplying $\\vec{x}_0$ by $\\mathbf{A}^{-1}$ $M$ times, we have\n",
    "\\begin{equation}\n",
    "\\left(\\mathbf{A}^{-1}\\right)^M \\vec{x}_0 = \\sum_{i=1}^N \\dfrac{\\alpha_i}{\\lambda_i^M} \\vec{u}_i.\n",
    "\\end{equation}\n",
    "\n",
    "Instead of converging to the eigenvector with the eigenvalue $\\lambda_i$ with the largest absolute magnitude, we find the power method using $\\mathbf{A}^{-1}$ converging to the eigenvector associated with the eigenvalue $\\lambda_i$ with the smallest absolute magnitude.\n",
    "\n",
    "In practice, given $\\mathbf{A}$ we do not proceed to evaluate $\\mathbf{A}^{-1}$ (which can be done using Gaussian elimination). Instead, we write the iteration equation as\n",
    "\\begin{equation}\n",
    "\\mathbf{A}\\vec{x}_{i+1} = \\vec{x}_i.\n",
    "\\end{equation}\n",
    "This means that instead of solving for $\\vec{x}_{i+1}$ from $\\vec{x}_i$ using matrix-vector multiplication, we solve for $\\vec{x}_{i+1}$ from $\\vec{x}_i$ using Gaussian elimination.\n",
    "\n",
    "Nevertheless, because this method is essentially the power method applied to the inverse of $\\mathbf{A}$, it is called the <em>inverse power method</em>.\n",
    "\n",
    "[Python code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 [[-0.24296159]\n",
      " [ 0.12422429]\n",
      " [-0.74905811]\n",
      " [ 0.60369689]]\n",
      "92 [[-0.56820305]\n",
      " [-0.30076265]\n",
      " [-0.76588571]\n",
      " [ 0.01030583]]\n",
      "93 [[-0.11508994]\n",
      " [-0.402638  ]\n",
      " [ 0.4146511 ]\n",
      " [-0.80789938]]\n",
      "94 [[ 0.24520029]\n",
      " [-0.12208006]\n",
      " [ 0.75047624]\n",
      " [-0.60146379]]\n",
      "95 [[  5.70548942e-01]\n",
      " [  3.06204104e-01]\n",
      " [  7.62045141e-01]\n",
      " [ -3.93637761e-04]]\n",
      "96 [[ 0.11072428]\n",
      " [ 0.39996211]\n",
      " [-0.41992692]\n",
      " [ 0.80711327]]\n",
      "97 [[-0.24743955]\n",
      " [ 0.11992934]\n",
      " [-0.75188481]\n",
      " [ 0.59921603]]\n",
      "98 [[-0.57283904]\n",
      " [-0.31166974]\n",
      " [-0.7580398 ]\n",
      " [-0.00964689]]\n",
      "99 [[-0.10640271]\n",
      " [-0.39729539]\n",
      " [ 0.42511971]\n",
      " [-0.80629279]]\n"
     ]
    }
   ],
   "source": [
    "# import numpy\n",
    "import numpy as np\n",
    "# initialize the matrix A\n",
    "A = np.array([[ 2., -3.,  1.,  3.],\n",
    "       [ 1.,  4., -3., -3.],\n",
    "       [ 5.,  3., -1., -1.],\n",
    "       [ 3., -6., -3.,  1.]])\n",
    "# choose an initial random vector x0\n",
    "x0 = np.random.random((4,1))\n",
    "# normalize x0\n",
    "x0 = x0/np.linalg.norm(x0)\n",
    "# start iteration\n",
    "x = x0\n",
    "for n in range(100):\n",
    "    x = np.linalg.solve(A,x)\n",
    "    x = x/np.linalg.norm(x)\n",
    "    if n > 90:\n",
    "        print(n, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shifted Inverse Power Method\n",
    "\n",
    "In the power method which targets the eigenvalue with the largest absolute value, there is no easy way to find other eigenvectors and eigenvalues. In the inverse power method, however, this is now possible.\n",
    "\n",
    "To see how this works, first observe that the matrix $\\mathbf{A} - \\alpha \\mathbf{I}$ has eigenvalues $\\lambda_i - \\alpha$. This matrix has the same eigenvectors $\\vec{u}_i$ as $\\mathbf{A}$. Let us suppose that for the matrix $\\mathbf{A}$, $\\lambda_1$ is the eigenvalue with the smallest absolute value, while $\\lambda_2$ is the eigenvalue with the second smallest absolute value.\n",
    "\n",
    "Then, with a suitable choice of $\\alpha$, it is possible to make $|\\lambda_2 - \\alpha| < |\\lambda_1 - \\alpha|$.\n",
    "\n",
    "When this condition is satisfied, the inverse power method will find $\\vec{u}_2$, the eigenvector associated with $\\lambda_2 - \\alpha$. Because of the shift by $\\alpha$, the method is called the <em>shifted inverse power method</em>.\n",
    "\n",
    "[Python code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QR Method\n",
    "\n",
    "While discussing how we can solve matrix equations, I introduced the LU decomposition, where any matrix $\\mathbf{A} = \\mathbf{L}\\mathbf{U}$ can be written as the product of a lower triangular matrix $\\mathbf{L}$ and an upper triangular matrix $\\mathbf{U}$.\n",
    "\n",
    "It turns out that it is also possible to write any matrix $\\mathbf{A}$ as the product\n",
    "\\begin{equation}\n",
    "\\mathbf{A} = \\mathbf{Q}\\mathbf{R}\n",
    "\\end{equation}\n",
    "of an orthogonal matrix $\\mathbf{Q}$, and an upper triangular matrix $\\mathbf{R}$. In fact, one might say that this <em>QR decomposition</em> of the matrix $\\mathbf{A}$ is more 'natural' than the LU decomposition, because it can be accomplished by applying a sequence of rotations to $\\mathbf{A}$. This is also why $\\mathbf{Q}$ is an orthogonal matrix.\n",
    "\n",
    "In case you forgot, an orthogonal matrix has the property\n",
    "\\begin{equation}\n",
    "\\mathbf{Q}\\mathbf{Q}^T = \\mathbf{I} = \\mathbf{Q}^T \\mathbf{Q}.\n",
    "\\end{equation}\n",
    "\n",
    "We are now ready to discuss the QR method for diagonalizing the matrix $\\mathbf{A}$. This method is voted as one of the top ten algorithms of the 20th century, but it is rather unintuitive.\n",
    "\n",
    "First, let us call $\\mathbf{A}_0 = \\mathbf{A}$ as our initial matrix. This matrix has the QR decomposition\n",
    "\\begin{equation}\n",
    "\\mathbf{A}_0 = \\mathbf{Q}_1 \\mathbf{R}_1.\n",
    "\\end{equation}\n",
    "\n",
    "Next, define\n",
    "\\begin{equation}\n",
    "\\mathbf{A}_1 = \\mathbf{R}_1 \\mathbf{Q}_1.\n",
    "\\end{equation}\n",
    "We call $\\mathbf{A}_1$ the <em>QR transform</em> of $\\mathbf{A}_0$.\n",
    "\n",
    "From $\\mathbf{A}_0 = \\mathbf{Q}_1 \\mathbf{R}_1$ we can write\n",
    "\\begin{equation}\n",
    "\\mathbf{Q}_1^T \\mathbf{A}_0 = \\mathbf{Q}_1^T \\mathbf{Q}_1 \\mathbf{R}_1 = \\mathbf{R}_1.\n",
    "\\end{equation}\n",
    "Substituting this into the expression for $\\mathbf{A}_1$, we find that\n",
    "\\begin{equation}\n",
    "\\mathbf{A}_1 = \\mathbf{R}_1 \\mathbf{Q}_1 = \\mathbf{Q}_1^T \\mathbf{A}_0 \\mathbf{Q}_1.\n",
    "\\end{equation}\n",
    "This tells us that $\\mathbf{A}_0$ and $\\mathbf{A}_1$ are related to each other through a <em>similarity transform</em>. Mathematicians tell us that if $\\mathbf{A}$ and $\\mathbf{B}$ are related to each other through a <em>similarity transform</em>, the two matrices have the same set of eigenvalues.\n",
    "\n",
    "In general, let us say that we have computed $\\mathbf{A}_k$ and is ready to compute $\\mathbf{A}_{k+1}$. We can find $\\mathbf{A}_{k+1}$ from\n",
    "\\begin{equation}\n",
    "\\mathbf{A}_{k+1} = \\mathbf{Q}_k^T \\mathbf{A}_k \\mathbf{Q}_k,\n",
    "\\end{equation}\n",
    "where $\\mathbf{A}_k = \\mathbf{Q}_k \\mathbf{R}_k$ and $\\mathbf{A}_{k+1} = \\mathbf{R}_k \\mathbf{Q}_k$. We can expand this expression as\n",
    "\\begin{equation}\n",
    "\\mathbf{A}_{k+1} = \\mathbf{Q}_k^T \\mathbf{A}_k \\mathbf{Q}_k = \\mathbf{Q}_k^T \\mathbf{Q}_{k-1}^T \\mathbf{A}_{k-1} \\mathbf{Q}_{k-1} \\mathbf{Q}_k = \\mathbf{Q}_k^T \\mathbf{Q}_{k-1}^T \\cdots \\mathbf{Q}_1^T\\mathbf{A}_0 \\mathbf{Q}_1 \\cdots \\mathbf{Q}_{k-1} \\mathbf{Q}_k.\n",
    "\\end{equation}\n",
    "\n",
    "In general, if $\\mathbf{Q}_1$ and $\\mathbf{Q}_2$ are orthogonal matrices, then the product $\\mathbf{Q}_1 \\mathbf{Q}_2$ is also an orthogonal matrix. Therefore, we can write\n",
    "\\begin{equation}\n",
    "\\mathbf{A}_{k+1} = \\mathbf{O}_k^T \\mathbf{A}_0 \\mathbf{O}_k,\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "\\mathbf{O}_k = \\mathbf{Q}_1 \\mathbf{Q}_2 \\cdots \\mathbf{Q}_k.\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, if we iterate infinitely many times, we would have\n",
    "\\begin{equation}\n",
    "\\mathbf{A}_{\\infty} = \\mathbf{O}_{\\infty}^T \\mathbf{A}_0 \\mathbf{O}_{\\infty}\n",
    "\\end{equation}\n",
    "\n",
    "Here comes the unintuitive bit: mathematicians can prove that $\\mathbf{A}_{\\infty}$ is an upper triangular matrix!\n",
    "\n",
    "QUESTION: What are the eigenvalues of an upper triangular matrix?\n",
    "\n",
    "[Answer here]\n",
    "\n",
    "The immense advantage the QR method has over the power method or the inverse power method is that we get all the eigenvalues in one fell swoop!\n",
    "\n",
    "In fact, we also get all the eigenvectors in one fell swoop, after the iteration converges!\n",
    "\n",
    "[Python exercise]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 2., -3.,  1.,  3.],\n",
       "        [ 1.,  4., -3., -3.],\n",
       "        [ 5.,  3., -1., -1.],\n",
       "        [ 3., -6., -3.,  1.]]),\n",
       " array([[  7.61788478e+00,   2.50063338e+00,  -1.30705666e+00,\n",
       "           3.04767343e+00],\n",
       "        [ -1.66137401e-13,  -3.92116067e+00,  -4.29500371e+00,\n",
       "           2.03764831e-01],\n",
       "        [ -4.59415084e-25,  -2.39888640e-11,  -1.53915921e+00,\n",
       "           3.57571306e+00],\n",
       "        [ -1.34483550e-25,  -3.79867555e-12,  -3.15195706e+00,\n",
       "           3.84243510e+00]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "At = np.array(A)\n",
    "for n in range(50):\n",
    "    # QR decomposition of At\n",
    "    Q, R = np.linalg.qr(At)\n",
    "    # QR transform of At\n",
    "    At = np.matmul(R, Q)\n",
    "A, At"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subspace Methods for Diagonalization\n",
    "\n",
    "Sometimes your matrix is so large that it takes a long time to completely diagonalize. Moreover, you only need a small number of eigenvectors and eigenvalues.\n",
    "\n",
    "When this is so, use subspace methods like the <em>Lanczos method</em> or the <em>Arnoldi</em> method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Diagonalization in Numpy\n",
    "\n",
    "As always, we learn the working principles behind various methods of matrix diagonalization so that we can hold our heads up high and call ourselves computational physicists.\n",
    "\n",
    "But in practice, we will not write functions that do these diagonalization, especially since numpy has built-in functions.\n",
    "\n",
    "[Python: use eig() to diagonalize A]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2., -3.,  1.,  3.],\n",
       "       [ 1.,  4., -3., -3.],\n",
       "       [ 5.,  3., -1., -1.],\n",
       "       [ 3., -6., -3.,  1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, u = np.linalg.eig(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.92116067+0.j        ,  1.15163794+2.00751206j,\n",
       "        1.15163794-2.00751206j,  7.61788478+0.j        ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.24040213+0.j        , -0.23920438-0.18985937j,\n",
       "        -0.23920438+0.18985937j,  0.60688104+0.j        ],\n",
       "       [ 0.43805314+0.j        ,  0.08358075-0.25397695j,\n",
       "         0.08358075+0.25397695j, -0.45920684+0.j        ],\n",
       "       [ 0.24593406+0.j        , -0.67290854+0.j        ,\n",
       "        -0.67290854-0.j        ,  0.11823560+0.j        ],\n",
       "       [ 0.83056167+0.j        ,  0.50257590-0.36035568j,\n",
       "         0.50257590+0.36035568j,  0.63784389+0.j        ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
